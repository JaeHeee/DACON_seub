{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T03:41:56.114613Z",
     "start_time": "2020-09-08T03:41:56.108396Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# import tensorflow_addons as tfa\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import SeparableConv2D, Input, Conv2D, Add, BatchNormalization, concatenate, AveragePooling2D, add, MaxPooling2D \n",
    "from tensorflow.keras.layers import Conv2DTranspose, Activation, Dropout,UpSampling2D ,ZeroPadding2D, LeakyReLU\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습 graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T03:41:56.486537Z",
     "start_time": "2020-09-08T03:41:56.480389Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2,figsize = (12, 8))\n",
    "    \n",
    "    for i in range(len(ax)):\n",
    "        ax[i].set_xlabel('Epochs')\n",
    "        ax[i].set_ylabel('Value')\n",
    "        \n",
    "        for n in range(len(list_of_metrics)):\n",
    "            if i == 0:\n",
    "                y = hist[list_of_metrics[n]]\n",
    "                if n == 0:\n",
    "                    ax[i].plot(epochs, y, label=\"train\")\n",
    "                else:\n",
    "                    ax[i].plot(epochs, y, label=\"val\")\n",
    "                ax[i].set_title('Loss')\n",
    "                ax[i].legend(loc='upper right')\n",
    "                if n == 1:\n",
    "                    break\n",
    "            else:\n",
    "                if n >= 2:\n",
    "                    y = hist[list_of_metrics[n]]\n",
    "                    if n == 2:\n",
    "                        ax[i].plot(epochs, y, label=\"train\")\n",
    "                    else:\n",
    "                        ax[i].plot(epochs, y, label=\"val\")\n",
    "                    ax[i].set_title('Accuracy')\n",
    "                    ax[i].legend(loc='lower right')\n",
    "                    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델이 학습한 후의 그래프를 보여줍니다.  \n",
    "정확도와 오차를 보기 쉽게 비교해줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alphabet image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T03:41:57.347228Z",
     "start_time": "2020-09-08T03:41:57.339252Z"
    }
   },
   "outputs": [],
   "source": [
    "def digit_image(x):\n",
    "    return np.where(x>=150, x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 이미지에서 숫자만 따로 추출하는 함수입니다.  \n",
    "숫자 전부가 아닌 알파벳 마스크 때문에 가려진 부분은 추출되지 않을 것 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T03:41:58.318315Z",
     "start_time": "2020-09-08T03:41:58.232134Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/data.npy', 'rb') as f:\n",
    "    X_trains= np.load(f)\n",
    "    y_trains = np.load(f)\n",
    "    TEST = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T03:44:22.784712Z",
     "start_time": "2020-09-08T03:44:22.782296Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 28, 28, 3)\n",
      "(2048, 10)\n",
      "(20480, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_trains.shape)\n",
    "print(y_trains.shape)\n",
    "print(TEST.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T03:44:23.106881Z",
     "start_time": "2020-09-08T03:44:23.024696Z"
    }
   },
   "outputs": [],
   "source": [
    "X_trains = X_trains.astype('float32')\n",
    "TEST = TEST.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주석 처리된 것처럼 함수형태로 만들어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T03:44:29.375636Z",
     "start_time": "2020-09-08T03:44:29.318003Z"
    }
   },
   "outputs": [],
   "source": [
    "# def rexnet_model(shape):\n",
    "#     inputs = Input(shape=(28,28,3))\n",
    "\n",
    "#     conv1 = tf.keras.layers.Conv2D(filters=128, kernel_size=(5,5), kernel_initializer='he_normal', activation='elu', padding='same',\n",
    "#                               kernel_constraint=tf.keras.constraints.max_norm(3.))(inputs) #28x28x128\n",
    "\n",
    "\n",
    "#     ## Residual Unit1\n",
    "#     batch1_1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "#     elu1_1 = tf.keras.layers.ELU()(batch1_1)\n",
    "#     res1_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(1,1), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu1_1) \n",
    "#     batch1_2 = tf.keras.layers.BatchNormalization()(res1_1)\n",
    "#     elu1_2 = tf.keras.layers.ELU()(batch1_2)\n",
    "#     res1_2 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu1_2) \n",
    "#     batch1_3 = tf.keras.layers.BatchNormalization()(res1_2)\n",
    "#     elu1_3 = tf.keras.layers.ELU()(batch1_3)\n",
    "#     res1_3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(1,1), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu1_3)                          \n",
    "\n",
    "#     # SEBlock1\n",
    "#     ch_input1 = 128\n",
    "#     ch_reduced1 = ch_input1//reduction_ratio\n",
    "#     avpool1 = tf.keras.layers.GlobalAveragePooling2D()(res1_3)\n",
    "#     sed1_1 = tf.keras.layers.Dense(ch_reduced1, kernel_initializer='he_normal', activation='elu', use_bias=False)(avpool1)\n",
    "#     sed1_2 = tf.keras.layers.Dense(ch_input1, kernel_initializer='he_normal', activation='sigmoid', use_bias=False)(sed1_1)\n",
    "#     reshape1 = tf.keras.layers.Reshape((1,1,ch_input1))(sed1_2)\n",
    "#     mul1 = tf.keras.layers.Multiply()([res1_3, reshape1])\n",
    "#     #\n",
    "\n",
    "#     skip1 = tf.keras.layers.Conv2D(filters=128, kernel_size=(1,1), padding='same')(conv1)\n",
    "#     res1 = tf.keras.layers.Add()([mul1, skip1]) #28x28x128\n",
    "#     ##\n",
    "\n",
    "#     ## Residual Unit2\n",
    "#     batch2_1 = tf.keras.layers.BatchNormalization()(res1)\n",
    "#     elu2_1 = tf.keras.layers.ELU()(batch2_1)\n",
    "#     res2_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(1,1), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu2_1) \n",
    "#     batch2_2 = tf.keras.layers.BatchNormalization()(res2_1)\n",
    "#     elu2_2 = tf.keras.layers.ELU()(batch2_2)\n",
    "#     res2_2 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu2_2) \n",
    "#     batch2_3 = tf.keras.layers.BatchNormalization()(res2_2)\n",
    "#     elu2_3 = tf.keras.layers.ELU()(batch2_3)\n",
    "#     res2_3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(1,1), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu2_3)                         \n",
    "\n",
    "\n",
    "#     # SEBlock2\n",
    "#     ch_input2 = 128\n",
    "#     ch_reduced2 = ch_input2//reduction_ratio\n",
    "#     avpool2 = tf.keras.layers.GlobalAveragePooling2D()(res2_3)\n",
    "#     sed2_1 = tf.keras.layers.Dense(ch_reduced2, kernel_initializer='he_normal', activation='elu', use_bias=False)(avpool2)\n",
    "#     sed2_2 = tf.keras.layers.Dense(ch_input2, kernel_initializer='he_normal', activation='sigmoid', use_bias=False)(sed2_1)\n",
    "#     reshape2 = tf.keras.layers.Reshape((1,1,ch_input2))(sed2_2)\n",
    "#     mul2 = tf.keras.layers.Multiply()([res2_3, reshape2])\n",
    "#     #\n",
    "#     skip2 = tf.keras.layers.Conv2D(filters=128, kernel_size=(1,1), padding='same')(res1)\n",
    "#     res2 = tf.keras.layers.Add()([mul2, skip2])\n",
    "#     drop2 = tf.keras.layers.Dropout(0.3)(res2) # 28x28x128\n",
    "#     ##\n",
    "\n",
    "\n",
    "#     ## Residual Unit3\n",
    "#     batch3_1 = tf.keras.layers.BatchNormalization()(drop2)\n",
    "#     elu3_1 = tf.keras.layers.ELU()(batch3_1)\n",
    "#     res3_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(1,1), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu3_1) \n",
    "#     batch3_2 = tf.keras.layers.BatchNormalization()(res3_1)\n",
    "#     elu3_2 = tf.keras.layers.ELU()(batch3_2)\n",
    "#     res3_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu3_2) \n",
    "#     batch3_3 = tf.keras.layers.BatchNormalization()(res3_2)\n",
    "#     elu3_3 = tf.keras.layers.ELU()(batch3_3)\n",
    "#     res3_3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(1,1), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu3_3)                           \n",
    "\n",
    "\n",
    "#     # SEBlock3\n",
    "#     ch_input3 = 128\n",
    "#     ch_reduced3 = ch_input3//reduction_ratio\n",
    "#     avpool3 = tf.keras.layers.GlobalAveragePooling2D()(res3_3)\n",
    "#     sed3_1 = tf.keras.layers.Dense(ch_reduced3, kernel_initializer='he_normal', activation='elu', use_bias=False)(avpool3)\n",
    "#     sed3_2 = tf.keras.layers.Dense(ch_input3, kernel_initializer='he_normal', activation='sigmoid', use_bias=False)(sed3_1)\n",
    "#     reshape3 = tf.keras.layers.Reshape((1,1,ch_input3))(sed3_2)\n",
    "#     mul3 = tf.keras.layers.Multiply()([res3_3, reshape3])\n",
    "#     #\n",
    "#     skip3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(1,1),padding='same')(drop2)\n",
    "#     res3 = tf.keras.layers.Add()([mul3, skip3]) #28x28x128\n",
    "#     ##\n",
    "\n",
    "\n",
    "#     ## Residual Unit4\n",
    "#     batch4_1 = tf.keras.layers.BatchNormalization()(res3)\n",
    "#     elu4_1 = tf.keras.layers.ELU()(batch4_1)\n",
    "#     res4_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(1,1), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu4_1) \n",
    "#     batch4_2 = tf.keras.layers.BatchNormalization()(res4_1)\n",
    "#     elu4_2 = tf.keras.layers.ELU()(batch4_2)\n",
    "#     res4_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu4_2) \n",
    "#     batch4_3 = tf.keras.layers.BatchNormalization()(res4_2)\n",
    "#     elu4_3 = tf.keras.layers.ELU()(batch4_3)\n",
    "#     res4_3 = tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu4_3)                              \n",
    "\n",
    "\n",
    "#     # SEBlock4\n",
    "#     ch_input4 = 256\n",
    "#     ch_reduced4 = ch_input4//reduction_ratio\n",
    "#     avpool4 = tf.keras.layers.GlobalAveragePooling2D()(res4_3)\n",
    "#     sed4_1 = tf.keras.layers.Dense(ch_reduced4, kernel_initializer='he_normal', activation='elu', use_bias=False)(avpool4)\n",
    "#     sed4_2 = tf.keras.layers.Dense(ch_input4, kernel_initializer='he_normal', activation='sigmoid', use_bias=False)(sed4_1)\n",
    "#     reshape4 = tf.keras.layers.Reshape((1,1,ch_input4))(sed4_2)\n",
    "#     mul4 = tf.keras.layers.Multiply()([res4_3, reshape4])\n",
    "#     #\n",
    "#     skip4 = tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1),padding='same')(res3)\n",
    "#     res4 = tf.keras.layers.Add()([mul4, skip4])\n",
    "#     pool4 = tf.keras.layers.MaxPool2D((2,2))(res4)\n",
    "#     drop4 = tf.keras.layers.Dropout(0.3)(pool4) # 14x14x256\n",
    "#     ##\n",
    "\n",
    "\n",
    "#     ## Residual Unit5\n",
    "#     batch5_1 = tf.keras.layers.BatchNormalization()(drop4)\n",
    "#     elu5_1 = tf.keras.layers.ELU()(batch5_1)\n",
    "#     res5_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(1,1), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu5_1) \n",
    "#     batch5_2 = tf.keras.layers.BatchNormalization()(res5_1)\n",
    "#     elu5_2 = tf.keras.layers.ELU()(batch5_2)\n",
    "#     res5_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu5_2) \n",
    "#     batch5_3 = tf.keras.layers.BatchNormalization()(res5_2)\n",
    "#     elu5_3 = tf.keras.layers.ELU()(batch5_3)\n",
    "#     res5_3 = tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu5_3)                                 \n",
    "\n",
    "\n",
    "#     # SEBlock5\n",
    "#     ch_input5 = 256\n",
    "#     ch_reduced5 = ch_input5//reduction_ratio\n",
    "#     avpool5 = tf.keras.layers.GlobalAveragePooling2D()(res5_3)\n",
    "#     sed5_1 = tf.keras.layers.Dense(ch_reduced5, kernel_initializer='he_normal', activation='elu', use_bias=False)(avpool5)\n",
    "#     sed5_2 = tf.keras.layers.Dense(ch_input5, kernel_initializer='he_normal', activation='sigmoid', use_bias=False)(sed5_1)\n",
    "#     reshape5 = tf.keras.layers.Reshape((1,1,ch_input5))(sed5_2)\n",
    "#     mul5 = tf.keras.layers.Multiply()([res5_3, reshape5])\n",
    "#     #\n",
    "#     skip5 = tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1),padding='same')(drop4)\n",
    "#     res5 = tf.keras.layers.Add()([mul5, skip5])\n",
    "#     pool5 = tf.keras.layers.MaxPool2D((2,2))(res5) #7x7x256\n",
    "#     ##\n",
    "\n",
    "\n",
    "#     ## Residual Unit6\n",
    "#     batch6_1 = tf.keras.layers.BatchNormalization()(pool5)\n",
    "#     elu6_1 = tf.keras.layers.ELU()(batch6_1)\n",
    "#     res6_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(1,1), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu6_1) \n",
    "#     batch6_2 = tf.keras.layers.BatchNormalization()(res6_1)\n",
    "#     elu6_2 = tf.keras.layers.ELU()(batch6_2)\n",
    "#     res6_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu6_2) \n",
    "#     batch6_3 = tf.keras.layers.BatchNormalization()(res6_2)\n",
    "#     elu6_3 = tf.keras.layers.ELU()(batch6_3)\n",
    "#     res6_3 = tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu6_3)\n",
    "#     # SEBlock6\n",
    "#     ch_input6 = 256\n",
    "#     ch_reduced6 = ch_input6//reduction_ratio\n",
    "#     avpool6 = tf.keras.layers.GlobalAveragePooling2D()(res6_3)\n",
    "#     sed6_1 = tf.keras.layers.Dense(ch_reduced6, kernel_initializer='he_normal', activation='elu', use_bias=False)(avpool6)\n",
    "#     sed6_2 = tf.keras.layers.Dense(ch_input6, kernel_initializer='he_normal', activation='sigmoid', use_bias=False)(sed6_1)\n",
    "#     reshape6 = tf.keras.layers.Reshape((1,1,ch_input6))(sed6_2)\n",
    "#     mul6 = tf.keras.layers.Multiply()([res6_3, reshape6])\n",
    "#     #\n",
    "#     skip6 = tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1),padding='same')(pool5)\n",
    "#     res6 = tf.keras.layers.Add()([mul6, skip6])\n",
    "#     drop6 = tf.keras.layers.Dropout(0.3)(res6) # 7x7x256\n",
    "#     ##\n",
    "\n",
    "#     ## Residual Unit7\n",
    "#     batch7_1 = tf.keras.layers.BatchNormalization()(drop6)\n",
    "#     elu7_1 = tf.keras.layers.ELU()(batch7_1)\n",
    "#     res7_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(1,1), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu7_1) \n",
    "#     batch7_2 = tf.keras.layers.BatchNormalization()(res7_1)\n",
    "#     elu7_2 = tf.keras.layers.ELU()(batch7_2)\n",
    "#     res7_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu7_2) \n",
    "#     batch7_3 = tf.keras.layers.BatchNormalization()(res7_2)\n",
    "#     elu7_3 = tf.keras.layers.ELU()(batch7_3)\n",
    "#     res7_3 = tf.keras.layers.Conv2D(filters=512, kernel_size=(1,1), padding='same',kernel_constraint=tf.keras.constraints.max_norm(3.))(elu7_3)\n",
    "#     skip7 = tf.keras.layers.Conv2D(filters=512, kernel_size=(1,1),padding='same')(drop6)\n",
    "#     res7 = tf.keras.layers.Add()([res7_3, skip7])\n",
    "#     pool7 = tf.keras.layers.MaxPool2D((3,3))(res7) \n",
    "    \n",
    "#     flatten1 = tf.keras.layers.Flatten()(pool7)\n",
    "    \n",
    "#     dense1 = tf.keras.layers.Dense(1024, activation='elu', kernel_initializer='he_normal', kernel_constraint=tf.keras.constraints.max_norm(3.))(flatten1)\n",
    "    \n",
    "#     drop1 = tf.keras.layers.Dropout(0.5)(dense1)\n",
    "#     outputs = tf.keras.layers.Dense(10 ,activation='softmax')(drop1)\n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T03:44:30.082046Z",
     "start_time": "2020-09-08T03:44:30.064534Z"
    }
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(X_trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datagen = ImageDataGenerator(\n",
    "    rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T03:44:30.560025Z",
     "start_time": "2020-09-08T03:44:30.551077Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 많은 데이터 셋으로 학습시키기 위해 FOLD를 100으로 설정했습니다. Fold 1 중간에 Stop시켰기에 break 조건을 넣어놨습니다.\n",
    "def get_stacking(in_model, train, train_y, n_fold = 10, SEED=2020, epochs = 500, batch_size = 32):\n",
    "    k_fold = KFold(n_splits=8, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    \n",
    "    model_number = 0\n",
    "    history = []\n",
    "    val_predict = []\n",
    "\n",
    "    train_fold_predict = np.zeros((train.shape[0], 1))\n",
    "    test_predict = []\n",
    "    arm_test_predict = np.array([0]*204800).reshape(20480, 10).astype('float64')\n",
    "    \n",
    "    for cnt, (train_idx, val_idx) in enumerate(k_fold.split(train, train_y)):\n",
    "        print(f'*******************number_{cnt+1}_kfold_model*******************')\n",
    "        x_train, y_train = train[train_idx], train_y[train_idx]\n",
    "        x_val, y_val = train[val_idx], train_y[val_idx]\n",
    "        \n",
    "        model = in_model(train.shape[1:])\n",
    "        model.compile(loss=\"categorical_crossentropy\",\n",
    "                      optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07), \n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "        es = EarlyStopping(patience=15, verbose=1)\n",
    "        mc = ModelCheckpoint(f'./model/{in_model}_{cnt}.h5', save_best_only=True, verbose=1)\n",
    "        rlp = ReduceLROnPlateau(monitor='val_loss', patience=4, factor=0.8, min_lr=0.0001)\n",
    "        \n",
    "        history = model.fit(datagen.flow(x_train, y_train, shuffle=True, batch_size=batch_size),\n",
    "                                  epochs=epochs,\n",
    "                                  validation_data=(x_val, y_val),\n",
    "                                  verbose=1,\n",
    "                                  steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                                  validation_steps=x_val.shape[0] // batch_size,\n",
    "                                  callbacks=[es, mc, rlp]\n",
    "                                  )\n",
    "        \n",
    "        \n",
    "        plot_curve(history.epoch, history.history, ['loss', 'val_loss', 'accuracy', 'val_accuracy'])\n",
    "        \n",
    "        \n",
    "        model.load_weights(f'../model/{in_model}_best_{cnt}.h5')\n",
    "        pred_train = model.predict(x_val)\n",
    "        pred_test = model.predict(TEST)\n",
    "        \n",
    "        train_fold_predict[val_idx, :] = np.argmax(pred_train, 1).reshape((x_val.shape[0], 1))\n",
    "        \n",
    "        test_predict.append(pred_test)\n",
    "    \n",
    "    for pred_test in test_predict:\n",
    "        arm_test_predict += pred_test\n",
    "          \n",
    "    test_fold_predict = np.argmax(arm_test_predict, axis=1).reshape((TEST.shape[0], 1))\n",
    "    train_fold_predict = train_fold_predict.astype('int64')\n",
    "    \n",
    "    return train_fold_predict, test_fold_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 설정해주세요\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T04:13:09.959848Z",
     "start_time": "2020-09-08T03:44:31.628436Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_train, model_test = get_stacking(위에서 만든 모델함수, X_trains, y_trains, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle 파일 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format안에 이름을 설정해주세요  \n",
    "e.g.  \n",
    "rx_train_jh  \n",
    "rx_test_jh  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T04:25:45.988978Z",
     "start_time": "2020-09-08T04:25:45.979101Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(model_train, open(\"../new_data/{}.pkl\".format(이름을 설정해주세요), \"wb\"))\n",
    "pickle.dump(model_test, open(\"../new_data/{}.pkl\".format(이름을 설정해주세요), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
